# -*- coding: utf-8 -*-
"""ViT_Test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DcGSPIK4PDSIAIrdLlt1Y0MUK0o37bmP
"""

import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import os
import time
import random
import numpy as np
import pandas as pd
from PIL import Image
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torchinfo import summary
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split, TensorDataset
from math import cos, pi
from transformers import ViTForImageClassification, ViTFeatureExtractor
from transformers import ViTModel, ViTConfig
from torch.nn.functional import cross_entropy

class PatchEmbedding(nn.Module):
    def __init__(self, in_channels=3, patch_size=16, embedding_dim=768):
        super().__init__()
        self.patcher = nn.Conv2d(in_channels=in_channels, out_channels=embedding_dim,
                                 kernel_size=patch_size, stride=patch_size)
        self.flatten = nn.Flatten(start_dim=2, end_dim=3)

    def forward(self, x):
        x_patched = self.patcher(x)
        x_flattened = self.flatten(x_patched)
        return x_flattened.permute(0, 2, 1)  # [batch_size, num_patches, embedding_dim]

class MultiheadSelfAttentionBlock(nn.Module):
    def __init__(self, embedding_dim=768, num_heads=12, dropout_rate=0.5):
        super().__init__()
        self.layer_norm = nn.LayerNorm(embedding_dim)
        self.multihead_attn = nn.MultiheadAttention(embedding_dim, num_heads, batch_first=True)
        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer

    def forward(self, x):
        x = self.layer_norm(x)
        attn_output, _ = self.multihead_attn(query=x, key=x, value=x)
        x = self.dropout(attn_output)
        return x

class MLPBlock(nn.Module):
    def __init__(self, embedding_dim=768, mlp_size=3072, dropout_rate=0.2):
        super().__init__()
        self.layer_norm = nn.LayerNorm(embedding_dim)
        self.mlp = nn.Sequential(
            nn.Linear(embedding_dim, mlp_size),
            nn.GELU(),
            nn.Dropout(dropout_rate),
            nn.Linear(mlp_size, embedding_dim)
        )

    def forward(self, x):
        x_norm = self.layer_norm(x)
        return self.mlp(x_norm)

class TransformerEncoderBlock(nn.Module):
    def __init__(self, embedding_dim=768, num_heads=12, mlp_size=3072, dropout_rate=0.2):
        super().__init__()
        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim, num_heads, dropout_rate)
        self.mlp_block = MLPBlock(embedding_dim, mlp_size, dropout_rate)

    def forward(self, x):
        # Residual connection around MSA
        x = self.msa_block(x) + x
        # Residual connection around MLP
        x = self.mlp_block(x) + x
        return x

class ViT(nn.Module):
    def __init__(self, num_classes=18, embedding_dim=768, num_heads=12,
                 num_layers=12, image_size=224, patch_size=16):
        super(ViT, self).__init__()
        self.patch_embed = PatchEmbedding(in_channels=3, patch_size=patch_size, embedding_dim=embedding_dim)
        self.cls_token   = nn.Parameter(torch.randn(1, 1, embedding_dim)) # Class token
        self.pos_embed   = nn.Parameter(torch.randn(1, (image_size // patch_size)**2 + 1, embedding_dim))
        self.encoder_blocks = nn.ModuleList(
            [TransformerEncoderBlock(embedding_dim, num_heads) for _ in range(num_layers)]
        )
        self.classifier = nn.Linear(embedding_dim, num_classes)
        self.dropout = nn.Dropout(0.1)

    def forward(self, x):
        batch_size = x.shape[0]
        x = self.patch_embed(x)

        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        x = torch.cat((cls_tokens, x), dim=1)

        x = x + self.pos_embed
        x = self.dropout(x)

        for block in self.encoder_blocks:
            x = block(x)

        # Take the CLS token for classification
        x = x[:, 0]
        x = self.classifier(x)
        return x

# ------------------------------
# 5) Pretrained Weight Loading
# ------------------------------
def initialize_scratch_vit_with_pretrained(model_name, num_classes, device):
    # Load pre-trained config + weights
    pretrained_config = ViTConfig.from_pretrained(model_name)
    pretrained_vit = ViTModel.from_pretrained(model_name, config=pretrained_config)

    # Create your custom ViT
    scratch_vit = ViT(
        num_classes=num_classes,
        embedding_dim=pretrained_config.hidden_size,
        num_heads=pretrained_config.num_attention_heads,
        num_layers=pretrained_config.num_hidden_layers,
        image_size=224,
        patch_size=16
    )

    # Merge state dict
    scratch_dict = scratch_vit.state_dict()
    pretrained_dict = pretrained_vit.state_dict()

    # Filter out unnecessary keys
    filtered_pretrained_dict = {
        k: v for k, v in pretrained_dict.items()
        if k in scratch_dict and "classifier" not in k and "pooler" not in k
    }
    scratch_dict.update(filtered_pretrained_dict)
    scratch_vit.load_state_dict(scratch_dict)

    return scratch_vit.to(device)

# Instantiate model
model_name = "google/vit-base-patch16-224-in21k"
num_classes = 18
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model2 = initialize_scratch_vit_with_pretrained(model_name, num_classes, device)

test_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

class TestImageDataset(Dataset):
    def __init__(self, image_dir, transform):
        self.image_dir = image_dir
        self.transform = transform
        self.image_paths = [os.path.join(image_dir, img) for img in os.listdir(image_dir) if img.endswith(('.png', '.jpg', '.jpeg'))]

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        image = Image.open(image_path).convert('RGB')
        if self.transform:
            image = self.transform(image)
        return image, image_path

test_image_dir = 'AnimalTestData/AnimalTestData'
test_dataset = TestImageDataset(image_dir=test_image_dir, transform=test_transform)

test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

def predict_model(model, test_loader, device, class_mapping):
    model.eval()
    predictions = []
    image_paths = []

    with torch.no_grad():
        for images, paths in test_loader:
            images = images.to(device)

            outputs = model(images)
            _, predicted = torch.max(outputs, 1)

            predictions.extend(predicted.cpu().numpy())
            image_paths.extend(paths)

    inv_class_mapping = {v: k for k, v in class_mapping.items()}
    predicted_labels = [inv_class_mapping[pred] for pred in predictions]
    return image_paths, predicted_labels

class_mapping = {
    'beaver': 0, 'butterfly': 1, 'cougar': 2, 'crab': 3, 'crayfish': 4, 'crocodile': 5,
    'dolphin': 6, 'dragonfly': 7, 'elephant': 8, 'flamingo': 9, 'kangaroo': 10, 'leopard': 11,
    'llama': 12, 'lobster': 13, 'octopus': 14, 'pigeon': 15, 'rhino': 16, 'scorpion': 17
}

model_path = "vit_model_final2.pth"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model2.load_state_dict(torch.load(model_path))
model2.to(device)

image_paths, predicted_labels = predict_model(model2, test_loader, device, class_mapping)

output_df = pd.DataFrame({'ImagePath': image_paths, 'PredictedLabel': predicted_labels})
output_df.to_csv('test_predictions2.csv', index=False)
print("Predictions saved to test_predictions.csv")